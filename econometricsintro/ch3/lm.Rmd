---
title: 线性回归分析：原理、建模与诊断
author:
 - 授课教师：吴翔 \newline
 - 邮箱：wuhsiang@hust.edu.cn
date: "March 20, 2021"
linestretch: 1.25
fontsize: 18
header-includes:
  - \usepackage{ctex}
output:
  beamer_presentation:
    theme: "CambridgeUS"
    colortheme: "beaver"
    latex_engine: xelatex
    toc: true
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
options(digits = 2)

```


# 线性回归概述

## 简单案例

考虑智力测验成绩$x$、教育年限$z$和年收入$y$（万元）之间的关系。数据生成过程（**data generating process**, DGP） $y = -0.5 + 0.2 \cdot x$得到的样本。

```{r}

# clear directory
rm(list = ls())
set.seed(123)

```


```{r, echo = TRUE}

# generate dataset
x <- rnorm(n = 200, mean = 110, sd = 10)
beta <- c(-0.5, 0.2)
y <- beta[1] + beta[2] * x + rnorm(n = 200, mean = 0, sd = 0.5)
z <- round(-2 + 0.1 * x + rnorm(n = 200, mean = 0, sd = 0.4))
dat <- data.frame(x = x, y = y, z = z)

```

## 回归分析

```{r}

# linear regression
fit1 <- lm(y ~ x, data = dat)
summary(fit1)$coef

```

考虑$x$对$y$的效应，线性模型$R^{2} = `r summary(fit1)$r.squared`$，预测值$\hat{\beta} = (`r fit1$coef[1]`, `r fit1$coef[2]`)$接近实际值$\beta = (-0.5, 0.2)$。

```{r}

fit2 <- lm(y ~ z, data = dat)
summary(fit2)$coef

```

考虑$z$对$y$的效应，$y = `r fit2$coef[1]` + `r fit2$coef[2]` z$，且$R^{2} = `r summary(fit2)$r.squared`$。


## 虚假 vs 真实效应

```{r, echo = TRUE}

# linear regression
fit3 <- lm(y ~ x + z, data = dat)
summary(fit3)$coef

```

考虑模型$y = \beta_{0} + \beta_{1} x + \beta_{2} z$。结果显示，$y = `r fit3$coef[1]` + `r fit3$coef[2]` x$，且$R^{2} = `r summary(fit3)$r.squared`$。

**课堂思考: z对y的效应，是否显著？**


## 正效应 vs 负效应？

```{r, echo = TRUE}

# add a sample
dat1 <- rbind(dat, c(160, -100, 10))
fit4 <- lm(y ~ x, data = dat1)
summary(fit4)$coef

```

增加一个样本$c(160, -100, 10)$，重新考虑$x$对$y$的效应，$R^{2} = `r summary(fit4)$r.squared`$，预测值$\hat{\beta} = (`r fit4$coef[1]`, `r fit4$coef[2]`)$大幅偏离实际值$\beta = (-0.5, 0.2)$。

**课堂思考: x对y的效应，到底是正还是负？**

## 遗漏了变量？

![Dissertation defense](figures/defense.jpg){width=40%}

我：我的统计模型是。。。估计了体育锻炼和抑郁症状的关系

答辩委员：影响抑郁症状的因素非常多，很多变量都没有纳入，这个模型是有问题的！

**课堂思考: 做统计模型，到底需要纳入多少变量？**

## 如何学习线性回归？


![Master & PhD students who are learning regression models](figures/confused.jpg){width=40%}

**理念**：

- 方便有多门，归元无二路：原理 + 建模 + 诊断 >> 软件操作
- 挽弓当挽强，用箭当用长

## 课程存储地址

- 课程存储地址： [https://github.com/wuhsiang/Courses](https://github.com/wuhsiang/Courses)
- 资源：课件、案例数据及代码

![课程存储地址](../../QR.png){width=40%}


## 参考教材

- 谢宇. 回归分析. 北京：社会科学文献出版社. 2010.
- 威廉·贝里. 理解回归假设. 上海:格致出版社. 2012.
- 欧文·琼斯. R语言的科学编程与仿真. 西安：西安交通大学出版社. 2014.


# 线性回归原理

## 缘起

返回《物种起源》一书的年代，考虑遗传、变异与个体差异的问题：

- 随着物种的变异，其个体差异是否会一直增大？
- 个体差异上的**两极分化**是否是一般规律？
- 遗传与个体差异有无关系？

## Galton的身高研究

```{r, fig.align='center', fig.height=3, fig.width=3}

rm(list = ls())
# load Galton's height data
suppressMessages(library(UsingR))
suppressMessages(library(ggplot2))
data("galton")
# scatter plot
ggplot(galton, aes(x = parent, y = child)) + geom_point() + geom_smooth(method=lm, color="darkred", fill="blue") + theme_bw() + xlim(60, 75) + ylim(60, 75) + geom_abline(intercept = 0, slope = 1)

```

## 什么是“回归”？

Galton的身高研究发现：

- 父代的身高增加时，子代的身高也倾向于增加。
- 当父代高于平均身高时，子代身高比他更高的概率要小于比他更矮的概率；父代矮于平均身高时，子代身高比他更矮的概率要小于比他更高的概率。
- 同一族群中，子代的身高通常介于其父代的身高和族群的平均身高之间。

**回归效应**：

- 向平均数方向的回归 (regression toward mediocrity)，或曰：回归平庸
- 天之道，损有余而补不足

## Galton的开创性研究

Francis Galton（以及Karl Pearson）研究

- 个体差异：确立了社会科学研究与自然科学研究的根本区别
- 遗传与个体差异的关系：倡导“优生学”
- 双生儿法（twin method）：匹配方法（matching）之先河

**课堂讨论: 匹配方法和随机对照试验方法的区别（轮回）？**

## 社会科学定量研究逻辑

社会科学定量研究与自然科学定量研究的区别：

- 核心区别：变异（variation） vs 共相（universal，相对应的是殊相particular）
- 结论：或然性 vs 必然性
- 方法：归纳法 vs 演绎法
- 特征：普适规律 vs 特定**情境**下的规律

因而，社会科学定量研究即是，在特定的**社会（或管理）情境**，选取合宜的解释变量，以尽可能理解总体中结果变量的变异的来源。

## 理解回归的三种视角

回归模型考虑解释变量$x$与结果变量$y$的关系，
$$
y_{i} = f(X_{i}) + \epsilon_{i} = \beta X_{i} + \epsilon_{i}
$$
将观测值$y_{i}$分为结构部分$f(X_{i})$和随机部分$\epsilon_{i}$，并可以从**三个视角**来理解：

- **因果性**（计量经济领域）：观测项 = 机制项 + 干扰项
- **预测性**（机器学习领域）：观测项 = 预测项 + 误差项
- **描述性**（统计领域）：观测项 = 概括项 + 残差项

## 回归模型设定

考虑吸烟量$x$与血压$y$的关系，回归模型为：
$$
y_{i} = \alpha + \beta x_{i} + \epsilon_{i}.
$$

**暗含的假设**：

- A1. 线性假设（$E(y|x) = \beta x$）：非线性模型、结构模型
- A2. 同质性假设：随机参数/效应模型、分层线性模型


## 总体回归方程

给定$x = x^{k}$，在的$\epsilon_{i} \text{ i.i.d } \sim N(0, \sigma^{2})$假定下，对回归模型求条件期望得到如下**总体回归方程**，

$$
E(y|x = x^{k}) = \mu_{y|x^{k}} = \alpha + \beta x^{k}.
$$

含义：

- 给定任意$x^{k}$，对应的$y^{k} \sim N(\mu_{y|x^{k}}, \sigma^{2})$。
- 回归线穿过$(x^{k},  \mu_{y|x^{k}})$。
- 参数$\beta$刻画了$x$的变化对$y$的**条件期望**的影响。

## 总体回归线

![总体回归线](figures/condmean.png){width=60%}

**简化示例: 虚拟变量 --> 多分类变量 --> 连续变量**

## 暗含的假设

- A3. 独立同分布假设：
    - $E(\epsilon_{i}) = 0$：随机效应模型中的随机截距参数
    - $Cov(\epsilon_{i}, \epsilon_{j}) = 0$：时间序列模型、空间计量模型、嵌套模型
    - $\sigma_{i} = \sigma$：异方差问题
- A4. 关于$y$的假设：
    - $y$应是连续变量：广义线性模型
    - $y$的条件期望$\mu_{y|x^{k}} = E(y|x = x^{k})$符合正态分布：分位数回归
- A5. \textcolor{red}{正交（严格外生）假设}
    - 误差项$\epsilon$和$x$不相关，即$Cov(x, \epsilon) = 0$
    - 内生性问题

## 参数估计

普通最小二乘法（ordinary least squares, OLS）通过最小化残差平方和（扩展到多元回归的情境$y = \beta X + \epsilon$）估计参数：
$$
\text{min } SSE = \text{min} \sum_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2} = \sum_{i=1}^{n} (y_{i} - \beta X_{i})^{2}
$$
由偏导公式
$$
\frac{\partial SSE}{\partial \beta} = 0
$$
得到参数估计值
$$
\hat{b} = (X'X)^{-1}X'y.
$$

**课堂思考: (1)如何在熟悉的编程语言中，撰写函数估计多元线性模型？(2) 在实践中，OLS会造成什么缺陷？**

## 衡量估计方法

评判估计的黄金准则 （Fisher）：

- **无偏性**：在总体中进行$M$次抽样，$E[\hat{b}_{m}] = \beta$。
- **有效性**：在众多估计量中，$b$的抽样分布的方差最小。
- **一致性**：样本量增大时，$b$趋近于$\beta$。

**课堂思考：统计显著性与样本量有无关系？**

## 变异分解逻辑

样本观测值$y_{i}$、均值$\bar{y}$、预测值$\hat{y}$之间的关系

![变异的分解](figures/SSTdecomposition.jpg){width=60%}

**板书演示：变异分解逻辑**

## 变异分解公式

总平方和（sum of squares total, SST）可以分解为回归平方和（sum of squares regression, SSR）和残差平方和（sum of squares error, SSE）之和，

具体而言：
$$
\begin{aligned}
SST & = \sum_{i=1}^{n}(y_{i} - \bar{y})^{2} \\
    & = \sum_{i=1}^{n} [(y_{i} - \hat{y}_{i}) + (\hat{y}_{i} - \bar{y})]^{2} \\
    & = \sum_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2} + \sum_{i=1}^{n} (\hat{y}_{i} - \bar{y})^{2} \\
    & = SSE + SSR
\end{aligned}
$$

判定系数（coefficient of determination）$R^{2} = SSE / SST.$

**板书演示：变异分解推导**

## 多元线性回归与方差分析

假定多元线性模型中，待估计的参数个数为$p$，那么方差和自由度的分解如下：

- SST: 自由度为$n-1$
- SSE: 自由度为$n - p$
- SSR：自由度为$p - 1$

因而，自由度的分解为：
$$
n - 1 = (n - p) + (p - 1)
$$

**课堂思考: 假设模型有两个解释变量，其中$x_{1}$是连续变量，$x_{2}$是包含5个分类的分类变量，SSR的自由度为多少？**

## 方差分析表

变异来源 | 平方和  | 自由度  | 均方 |
---- | ---- | ----- | --- |
回归模型 | SSR | $p - 1$ | MSR = SSR/$(p-1)$ |
误差 | SSE | $n - p$ | MSE = SSE/$(n-p)$ |
总变异 | SST | $n-1$ | MST = SST/$(n-1)$

Table:多元线性回归的方差分析表

相应地，可以构造$F$检验：
$$
F(\text{df}_{\text{SSR}}, \text{df}_{\text{SSE}}) = \frac{\text{MSR}}{\text{MSE}} ?> F_{\alpha}
$$

**延伸内容：聚类分析**

## 模型选择

- 模型选择：**精确性原则** vs **简约性原则**
- 情境：假定在线性回归模型$A$的基础上，加了几个变量得到模型$B$，应当如何在模型A和B之间选择？

构造$F$检验：
$$
F(\Delta \text{df}, \text{df}_{\text{SSE}}) = \frac{\Delta \text{SSR} / \Delta \text{df}}{\text{MSE}_{\text{B}}} ?> F_{\alpha}
$$

## 遗漏变量问题

**遗漏变量（omitted variables）问题**是指，真实模型为：
$$
y \sim \beta_{1} x_{1} + \beta_{2} x_{2}. (1)
$$
但遗漏了变量$x_{2}$，且$Cov(x_{1}, x_{x}) \neq 0$，从而模型被错误设定为：
$$
y \sim \beta_{1} x_{1}. (2)
$$
记模型(1)的系数估计分别为$\hat{\beta_{1}}$和$\hat{\beta_{2}}$，模型(2)的系数估计为$\tilde{\beta_{1}}$，模型$x_{2} \sim \gamma x_{1}$的系数估计为$\hat{\gamma}$，那么，可以证明：
$$
\tilde{\beta_{1}} = \hat{\beta_{1}} + \hat{\beta_{2}} \cdot \hat{\gamma}.
$$

## 遗漏变量偏误

\textcolor{red}{遗漏变量偏误}（omitted variable bias, OVB）即为：
$$
\text{OVB} = \hat{\beta_{2}} \cdot \hat{\gamma}.
$$

**课堂讨论：（1）哪些变量一定要纳入模型，作为控制变量？（2）哪些变量不必纳入模型作为控制变量？（3）出现了遗漏变量，如何判断回归系数偏误的方向？**

# 线性回归建模

## 线性回归建模

本部分请参阅slides！

# 线性回归诊断

## 因变量分布与Box-Cox变换

当因变量不服从正态分布时，Box & Cox (1964)建议采用如下Box-Cox变换
$$
y_{i} =
\begin{cases}
[(y_{i}+\lambda_{2})^{\lambda_{1}} - 1] / \lambda_{1} & \text{ if } \lambda_{1} \neq 0, \\
ln(y_{i} + \lambda_{2}) & \text{ if } \lambda_{1} = 0.
\end{cases}
$$
将非正态的分布转换为正态分布。

**课堂思考: (1) 对数变换或Box-Cox变换是否合适？(2) 如何推导出“变化比例”这一含义？**


## 多重共线性

参数估计值
$$
\hat{\beta} = (X'X)^{-1}X'y
$$
要求$X'X$是**可逆（非奇异）**的。

- 完全多重共线性：模型无法识别
- 严重多重共线性：不影响估计的无偏性和一致性，损害参数估计的**有效性**，及标准误会增大
- 判断标准：**方差膨胀因子**（variance inflation factor, VIF）最大值超过10，平均值明显大于1

## 消除共线性

- $k$水平分类变量：虚拟变量（dummy variable）化后，只能有$k-1$个虚拟变量
- 减少解释变量个数
- 维度规约：因子分析
- 变量选择：如lasso等统计机器学习方法，尤其是$n < p$时模型无法识别的情形

```{r, echo = T, eval = F}

suppressMessages(library(car))
# calculate VIF
vif(fit)

```

## 异方差

通常将违背残差分布假定的

- 自相关：$\text{Cov}(\epsilon_{i}, \epsilon_{j}) \neq 0$
- 异方差：$\text{Var}(\epsilon_{i}) \neq \text{Var}(\epsilon_{j})$

统称为**异方差**。异方差不影响估计的无偏性和一致性，但会损害估计的**有效性**。


处理异方差的方法包括：

- 调整标准误的计算，采用稳健标准误
- 采用广义最小二乘法（generalized least squares, GLS）估计模型


## 处理非线性

- 纳入二次项：处理$U$型关系
- 采用对数项：处理比例关系
- 纳入交互项：处理调节作用

## 高影响点及异常值处理

OLS采用最小化误差**平方和**的方式，使估计值对异常值非常敏感

- **高影响点/高杠杆点**（influential/leverage points）：观测案例$i$对**回归系数**影响较大的点，通常可由Cook距离等统计量衡量
- **异常值**：模型拟合失败的观测点，它们大幅**偏离回归线**，通常由标准化残差来衡量（其绝对值不宜大于5）

因而需要识别高影响点和异常值，并**谨慎判断**是否要排除这些观测样本。

## 实践中的回归假设

1. 模型设定假设
    - 线性模型假设：$E(y|X) = \beta X + \epsilon$ （\textcolor{red}{可检验}）
    - 同质效应假设：$\beta_{i} = \beta$ （\textcolor{red}{可检验，高阶议题*}）
2. 正交假设 （\textcolor{blue}{OLS自动保证，不必检验}）
    - 误差项均值为0：$E(\epsilon) = 0$
    - 误差项与解释变量不相关：$\text{Cov}(X,\epsilon) = 0$
3. 独立同分布假设
    - 误差项相互独立：$\text{Cov}(\epsilon_{i}, \epsilon_{j}) = 0$
    - 误差项方差相同：$\text{Var}(\epsilon_{i}) = \sigma^{2}$ （\textcolor{red}{可检验}）
4. 正态分布假设 （\textcolor{blue}{大样本时，不必要}）
    - 误差项服从正态分布：$\epsilon_{i} \sim N(0, \sigma^{2})$

## 实践中的回归假设（续）

由回归模型设定、OLS估计衍生出来的问题：

1. 结果变量$y$的分布 （\textcolor{red}{可检验, Box-Cox变换}）
2. 多重共线性 （\textcolor{red}{可检验}）
3. 异常值 （\textcolor{red}{可检验}）

# 线性回归高阶议题（*）

## 内生性与异质性

考虑是否上大学（$D_{i} = 0, 1$）和收入的关系
$$
y_{i} = \alpha_{i} + \beta_{i} D_{i}.
$$

- **内生性**：匹配法（matching） vs 随机控制试验法（RCT）
- **异质性**：分层线性模型

## 贝叶斯视角

背景：

- Efron提出的bootstrapping方法
- 大数据时代的统计推断
- 频率学派 vs 贝叶斯学派

案例思考：

- 射击选手B，999/1000
- 射击选手A，100/100

## 案例思考

![灵犀一指 (999/1000) vs 小李飞刀 (100/100)](figures/bayesian.jpeg){width=60%}

## 先验的作用

情境一（先验8/10）：

- A：先验(8/10) + 数据（999/1000）-> 后验（1007/1010 = 0.9997） [**获胜**]
- B: 先验(8/10) + 数据（100/100）-> 后验（108/110 = 0.9818）


情境二（先验9999/10000）：

- A：先验(9999/10000) + 数据（999/1000）-> 后验（10998/11000 = 0.9998）
- B: 先验(9999/10000) + 数据（100/100）-> 后验（10099/10100 = 0.9999）[**获胜**]


## 回归分析总结

1. 回归假设与诊断：如何得到可靠的结论？
2. 变异及其分解：社会科学定量研究的核心
3. 高阶议题：计量经济应用的前沿
